#!/bin/bash
#
# script for mike
# 
# note: this ppn in the first line is not for mpi; it just is so that
# we have all the cores in each node available to us.
# we specify how many processors mpi will run on below, when we define np.
#PBS -l nodes=8:ppn=16
#PBS -A hpc_characters
#PBS -m e
#PBS -M Owen.Barrett@gmail.com
#PBS -q checkpt
#PBS -l walltime=00:10:00
#PBS -o char.o
#PBS -N char

date

# put output in our work directory
cd /work/obarrett
# Depending on the program called
# mkdir -p A2MultOut
# mkdir -p A2AddOut
# mkdir -p B2MultOut
# mkdir -p G2MultOut
# mkdir -p G2AddOut
# mkdir -p HessianMultOut
# mkdir -p HessianAddOut
# mkdir -p Kloosterman3Out
# mkdir -p A3MultOut
# mkdir -p B3MultOut

cat $PBS_NODEFILE > hosts
cat $PBS_NODEFILE | uniq > uniqhosts
nodes=$(cat uniqhosts | wc -l)

export OMP_NUM_THREADS=8
let np=2*nodes

# Depending on the program called
# mpirun -np $np -hostfile uniqhosts /work/obarrett/A2Mult 2 32 0 1 2 3
# mpirun -np $np -hostfile uniqhosts /work/obarrett/A2Add 2 32 0 1 2 3
# mpirun -np $np -hostfile uniqhosts /work/obarrett/B2Mult 2 32 0 1 2 3
# mpirun -np $np -hostfile uniqhosts /work/obarrett/G2Mult 2 32 0 1 4
# mpirun -np $np -hostfile uniqhosts /work/obarrett/G2Add 2 32 0 1 4
# mpirun -np $np -hostfile uniqhosts /work/obarrett/HessianMult 2 32 0 1 2
# mpirun -np $np -hostfile uniqhosts /work/obarrett/HessianAdd 2 32 0 1 2
# mpirun -np $np -hostfile uniqhosts /work/obarrett/Kloosterman3 2 32
# mpirun -np $np -hostfile uniqhosts /work/obarrett/A3Mult 2 32 0 1 4
# mpirun -np $np -hostfile uniqhosts /work/obarrett/B3Mult 2 32 0 1 2

date

exit 0
